{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3fd19a",
   "metadata": {},
   "source": [
    "# FloML - Example Analysis\n",
    "\n",
    "This notebook demonstrates the key capabilities of the FloML (Flood Machine Learning) package:\n",
    "\n",
    "1. **Segmented Linear Regression** - Model non-linear stage-discharge relationships\n",
    "2. **Multi-Station Correlation** - Analyze upstream-downstream timing\n",
    "3. **Precursor Detection** - Identify early flood warning signals\n",
    "\n",
    "Data is read from the PostgreSQL database curated by the Rust monitoring daemon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a6d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from floml.db import get_engine, verify_schemas\n",
    "from floml.regression import fit_segmented_regression, fit_stage_discharge\n",
    "from floml.correlation import correlate_stations, find_optimal_lag\n",
    "from floml.precursors import analyze_precursors, calculate_rise_rate\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c79960",
   "metadata": {},
   "source": [
    "## 1. Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40bdeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = get_engine()\n",
    "verify_schemas(engine)\n",
    "print(\"✓ Database connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdfa6c9",
   "metadata": {},
   "source": [
    "## 2. Load Available Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_sql(\"\"\"\n",
    "    SELECT site_code, site_name, latitude, longitude, monitored\n",
    "    FROM usgs_raw.sites\n",
    "    ORDER BY site_code\n",
    "\"\"\", engine)\n",
    "\n",
    "stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1e8ac1",
   "metadata": {},
   "source": [
    "## 3. Segmented Regression - Stage-Discharge Relationship\n",
    "\n",
    "Rivers often have non-linear stage-discharge curves due to:\n",
    "- Channel overflow into floodplain\n",
    "- Backwater effects\n",
    "- Ice jams\n",
    "\n",
    "We use piecewise linear fitting to model these regime changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71fc89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paired stage-discharge data for Kingston Mines\n",
    "site_code = '05568500'\n",
    "\n",
    "data = pd.read_sql(f\"\"\"\n",
    "    SELECT reading_time,\n",
    "           MAX(CASE WHEN parameter_code = '00065' THEN value END) as stage_ft,\n",
    "           MAX(CASE WHEN parameter_code = '00060' THEN value END) as discharge_cfs\n",
    "    FROM usgs_raw.gauge_readings\n",
    "    WHERE site_code = '{site_code}'\n",
    "      AND reading_time > NOW() - INTERVAL '1 year'\n",
    "    GROUP BY reading_time\n",
    "    HAVING MAX(CASE WHEN parameter_code = '00065' THEN value END) IS NOT NULL\n",
    "       AND MAX(CASE WHEN parameter_code = '00060' THEN value END) IS NOT NULL\n",
    "\"\"\", engine)\n",
    "\n",
    "print(f\"Loaded {len(data)} paired observations\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4cab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data) >= 50:\n",
    "    # Fit 3-segment model\n",
    "    result = fit_stage_discharge(data['discharge_cfs'], data['stage_ft'], n_segments=3)\n",
    "    \n",
    "    print(result)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(data['discharge_cfs'], data['stage_ft'], alpha=0.5, s=10, label='Observed')\n",
    "    \n",
    "    # Fitted line\n",
    "    x_range = np.linspace(data['discharge_cfs'].min(), data['discharge_cfs'].max(), 500)\n",
    "    y_pred = result.predict(x_range)\n",
    "    ax.plot(x_range, y_pred, 'r-', linewidth=2, label='Fitted (3 segments)')\n",
    "    \n",
    "    # Mark breakpoints\n",
    "    for bp in result.breakpoints[1:-1]:\n",
    "        ax.axvline(bp, color='green', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Discharge (cfs)')\n",
    "    ax.set_ylabel('Stage (ft)')\n",
    "    ax.set_title(f'Stage-Discharge Relationship: {site_code}\\nR² = {result.r_squared:.4f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data for regression analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca9eff",
   "metadata": {},
   "source": [
    "## 4. Multi-Station Correlation\n",
    "\n",
    "Analyze how upstream stations correlate with downstream, including timing lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2047d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stage data for two stations (if available)\n",
    "upstream_code = '05568500'  # Kingston Mines\n",
    "downstream_code = '05570000'  # Peoria\n",
    "\n",
    "# Note: You may need to adjust these site codes based on what's in your database\n",
    "print(f\"Analyzing correlation: {upstream_code} → {downstream_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "upstream_data = pd.read_sql(f\"\"\"\n",
    "    SELECT reading_time as timestamp, value as stage_ft\n",
    "    FROM usgs_raw.gauge_readings\n",
    "    WHERE site_code = '{upstream_code}'\n",
    "      AND parameter_code = '00065'\n",
    "      AND reading_time > NOW() - INTERVAL '90 days'\n",
    "    ORDER BY reading_time\n",
    "\"\"\", engine)\n",
    "\n",
    "downstream_data = pd.read_sql(f\"\"\"\n",
    "    SELECT reading_time as timestamp, value as stage_ft\n",
    "    FROM usgs_raw.gauge_readings\n",
    "    WHERE site_code = '{downstream_code}'\n",
    "      AND parameter_code = '00065'\n",
    "      AND reading_time > NOW() - INTERVAL '90 days'\n",
    "    ORDER BY reading_time\n",
    "\"\"\", engine)\n",
    "\n",
    "print(f\"Upstream: {len(upstream_data)} readings\")\n",
    "print(f\"Downstream: {len(downstream_data)} readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca325878",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(upstream_data) > 100 and len(downstream_data) > 100:\n",
    "    upstream_series = upstream_data.set_index('timestamp')['stage_ft']\n",
    "    downstream_series = downstream_data.set_index('timestamp')['stage_ft']\n",
    "    \n",
    "    # Find correlation\n",
    "    corr_result = correlate_stations(upstream_series, downstream_series)\n",
    "    print(corr_result)\n",
    "else:\n",
    "    print(\"Insufficient data for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e01549",
   "metadata": {},
   "source": [
    "## 5. Flood Precursor Detection\n",
    "\n",
    "Analyze a historical flood event to identify early warning signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbce988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a flood event\n",
    "events = pd.read_sql(\"\"\"\n",
    "    SELECT e.id, e.site_code, e.crest_time, e.peak_stage_ft, e.severity\n",
    "    FROM nws.flood_events e\n",
    "    WHERE e.crest_time IS NOT NULL\n",
    "    ORDER BY e.crest_time DESC\n",
    "    LIMIT 5\n",
    "\"\"\", engine)\n",
    "\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81221420",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(events) > 0:\n",
    "    # Analyze first event\n",
    "    event = events.iloc[0]\n",
    "    site_code = event['site_code']\n",
    "    crest_time = pd.Timestamp(event['crest_time'])\n",
    "    \n",
    "    # Load stage data 14 days before peak\n",
    "    window_start = crest_time - pd.Timedelta(days=14)\n",
    "    \n",
    "    stage_data = pd.read_sql(f\"\"\"\n",
    "        SELECT reading_time, value as stage_ft\n",
    "        FROM usgs_raw.gauge_readings\n",
    "        WHERE site_code = '{site_code}'\n",
    "          AND parameter_code = '00065'\n",
    "          AND reading_time BETWEEN '{window_start}' AND '{crest_time}'\n",
    "        ORDER BY reading_time\n",
    "    \"\"\", engine)\n",
    "    \n",
    "    if len(stage_data) > 0:\n",
    "        stage_series = stage_data.set_index('reading_time')['stage_ft']\n",
    "        \n",
    "        # Detect precursors\n",
    "        precursors = analyze_precursors(stage_series, crest_time)\n",
    "        \n",
    "        print(f\"\\nEvent: {site_code} on {crest_time}\")\n",
    "        print(f\"Peak: {event['peak_stage_ft']:.2f} ft ({event['severity']})\")\n",
    "        print(f\"\\nFound {len(precursors)} precursor events:\")\n",
    "        \n",
    "        for p in precursors:\n",
    "            print(f\"  • {p.precursor_type:15s} {p.hours_before_peak:6.1f}h before peak\")\n",
    "        \n",
    "        # Plot\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "        \n",
    "        # Stage hydrograph\n",
    "        ax1.plot(stage_series.index, stage_series.values, 'b-', linewidth=1)\n",
    "        ax1.axvline(crest_time, color='red', linestyle='--', label='Peak')\n",
    "        ax1.axhline(event['peak_stage_ft'], color='red', linestyle=':', alpha=0.5)\n",
    "        \n",
    "        # Mark precursors\n",
    "        for p in precursors:\n",
    "            ax1.axvline(p.detected_at, color='orange', alpha=0.3)\n",
    "        \n",
    "        ax1.set_ylabel('Stage (ft)')\n",
    "        ax1.set_title(f'Flood Event Analysis: {site_code}')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rise rate\n",
    "        rise_rate = calculate_rise_rate(stage_series)\n",
    "        ax2.plot(rise_rate.index, rise_rate.values, 'g-', linewidth=1)\n",
    "        ax2.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "        ax2.axhline(0.5, color='orange', linestyle='--', alpha=0.5, label='Rapid rise threshold')\n",
    "        ax2.set_ylabel('Rise Rate (ft/day)')\n",
    "        ax2.set_xlabel('Time')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No stage data available for this event\")\n",
    "else:\n",
    "    print(\"No flood events found in database\")\n",
    "    print(\"Run: cargo run --bin ingest_peak_flows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99462080",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **More Data**: Ingest historical data using the Rust daemon\n",
    "   ```bash\n",
    "   cd ../flomon_service\n",
    "   cargo run --bin historical_ingest\n",
    "   cargo run --bin ingest_peak_flows\n",
    "   ```\n",
    "\n",
    "2. **Custom Analysis**: Use FloML modules for your specific analysis\n",
    "\n",
    "3. **Write Results**: Store analysis results back to `flood_analysis` schema"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
